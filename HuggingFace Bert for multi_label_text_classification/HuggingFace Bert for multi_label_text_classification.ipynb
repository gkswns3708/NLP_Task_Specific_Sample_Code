{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z9owNsovzrs"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo3Z9nvXv3oU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daEN6WR_wApu"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3lUxIP5wCKz"
      },
      "outputs": [],
      "source": [
        "example = dataset['train'][0]\n",
        "example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74_kVst8wKzf"
      },
      "source": [
        "위 Dataset에는 여러가지 Feature가 존재합니다.\n",
        "\n",
        "이것을 이용해 ID와 Tweet 외의 다른 Column을 이용해 Multi-label Classification을 위한 Dataset을 구성해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7ukNl-OwKYq"
      },
      "outputs": [],
      "source": [
        "labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csczdYJUwFAN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "def preprocess_data(examples):\n",
        "    text = examples['Tweet']\n",
        "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128) # tokenizer로 dict형태의 Padding, Masking, token_type_ids 등등 다 만듦.\n",
        "    labels_batch = {k : examples[k] for k in examples.keys() if k in labels}\n",
        "    labels_matrix = np.zeros((len(text), len(labels)))\n",
        "\n",
        "    for idx, label in enumerate(labels):\n",
        "        labels_matrix[:, idx] = labels_batch[label]\n",
        "    \n",
        "    encoding['labels'] = labels_matrix.tolist()\n",
        "\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdlGAOyJxb8t"
      },
      "outputs": [],
      "source": [
        "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CLDJZNOxg6k"
      },
      "outputs": [],
      "source": [
        "example = encoded_dataset['train'][0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KvYZWgOx_l7"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTgwOxL9yBq3"
      },
      "outputs": [],
      "source": [
        "example['labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y_noay5yMxU"
      },
      "source": [
        "해당 Data에는 anticipation, optimizim, trust 가 포함되어 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXA6DyzlypK-"
      },
      "outputs": [],
      "source": [
        "example.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bji5S4oHyHRM"
      },
      "outputs": [],
      "source": [
        "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eWyBA93yTHN"
      },
      "source": [
        "마지막으로 우리는 우리의 Data의 Format을 PyTorch tensors로 설정해야한다. 이것은 PyTorch dataset의 training, validation,test로 변환될 예정이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kLiDGGbySoe"
      },
      "outputs": [],
      "source": [
        "encoded_dataset.set_format('torch') # tokenizer 형태로 변환된 Data를 torch.utils.data.Dataset형태로 변환함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kPddwfxPyImu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b41cc4-a50e-4a31-8d29-a8cbbe7bb3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"anger\",\n",
            "    \"1\": \"anticipation\",\n",
            "    \"2\": \"disgust\",\n",
            "    \"3\": \"fear\",\n",
            "    \"4\": \"joy\",\n",
            "    \"5\": \"love\",\n",
            "    \"6\": \"optimism\",\n",
            "    \"7\": \"pessimism\",\n",
            "    \"8\": \"sadness\",\n",
            "    \"9\": \"surprise\",\n",
            "    \"10\": \"trust\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"anger\": 0,\n",
            "    \"anticipation\": 1,\n",
            "    \"disgust\": 2,\n",
            "    \"fear\": 3,\n",
            "    \"joy\": 4,\n",
            "    \"love\": 5,\n",
            "    \"optimism\": 6,\n",
            "    \"pessimism\": 7,\n",
            "    \"sadness\": 8,\n",
            "    \"surprise\": 9,\n",
            "    \"trust\": 10\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"multi_label_classification\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", \n",
        "                                                           problem_type=\"multi_label_classification\", \n",
        "                                                           num_labels=len(labels),\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MaqHU2LkzB6l"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "metric_name = \"f1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "b7WQaEpFzJ_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3128ac3-e27e-42d2-cf24-9686b5696808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"bert-finetuned-sem_eval-english\", # output_dir\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    #push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gMEtisn43G-"
      },
      "source": [
        "우리는 Metric을 계산하기 위해 `compute_metric`함수를 정의할 겁니다. 이 함수는 metric 계산을 위한 Dicionary 값을 이용해 계산될 예정입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OCgU08Ls4z3E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "\n",
        "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
        "    # apply sigmoid on preditions which are of shape(batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1 # 6 ~ 9번 위치가 만족 -> (array([6, 7, 8, 9]))를 이용해 0, 1로 prediction을 나눔.\n",
        "    y_true = labels\n",
        "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred, average='micro')\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    metrics = { # 이런 Metric이 정해져 있는 듯 함.\n",
        "        'f1' : f1_micro_average,\n",
        "        'roc_auc' : roc_auc,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    result = multi_label_metrics (\n",
        "        predictions=preds,\n",
        "        labels=p.label_ids\n",
        "    )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nMXgbYtE6fUD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4dafaa9d-d6d6-4462-89ce-10b10a452ee5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "encoded_dataset['train'][0]['labels'].type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IAaVRGkv6gQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f8cf7f-c506-416c-8dc0-cb0c3987e26e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   101,    100, 102204,  31862,  10124,    169,  12935,  67701,  10135,\n",
              "           169,  18077,  13028,  11387,  14794,  10529,    112,    119,  35088,\n",
              "         12963,    119,    108,  63598,  11809,    108,  25121,    108,  12796,\n",
              "         31862,    102,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "encoded_dataset['train']['input_ids'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BGUgFeXX6heE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f9ab5f-85a0-4756-af22-14da64fb1ea0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('loss',\n",
              "                           tensor(0.6869, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)),\n",
              "                          ('logits',\n",
              "                           tensor([[-0.1150,  0.0214,  0.0988,  0.0486, -0.0101,  0.1667,  0.0157, -0.2099,\n",
              "                                    -0.0566, -0.0968, -0.0454]], grad_fn=<AddmmBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "#forward pass\n",
        "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bD69ufVp6ixX"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9aHwTF406w_S",
        "outputId": "8a8f1400-3237-4de2-9d07-072632cb2482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 6838\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 4275\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4275' max='4275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4275/4275 11:21, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Roc Auc</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.429600</td>\n",
              "      <td>0.358450</td>\n",
              "      <td>0.591216</td>\n",
              "      <td>0.720465</td>\n",
              "      <td>0.209932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.321100</td>\n",
              "      <td>0.346114</td>\n",
              "      <td>0.655164</td>\n",
              "      <td>0.766470</td>\n",
              "      <td>0.264108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.274300</td>\n",
              "      <td>0.333408</td>\n",
              "      <td>0.669056</td>\n",
              "      <td>0.773468</td>\n",
              "      <td>0.267494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.249100</td>\n",
              "      <td>0.336251</td>\n",
              "      <td>0.677582</td>\n",
              "      <td>0.780612</td>\n",
              "      <td>0.266366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.218300</td>\n",
              "      <td>0.342180</td>\n",
              "      <td>0.676245</td>\n",
              "      <td>0.782141</td>\n",
              "      <td>0.248307</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 886\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-855\n",
            "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-855/config.json\n",
            "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-855/pytorch_model.bin\n",
            "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-855/tokenizer_config.json\n",
            "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-855/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 886\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1710\n",
            "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1710/config.json\n",
            "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1710/pytorch_model.bin\n",
            "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1710/tokenizer_config.json\n",
            "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1710/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 886\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2565\n",
            "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2565/config.json\n",
            "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2565/pytorch_model.bin\n",
            "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2565/tokenizer_config.json\n",
            "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2565/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 886\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3420\n",
            "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3420/config.json\n",
            "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3420/pytorch_model.bin\n",
            "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3420/tokenizer_config.json\n",
            "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3420/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 886\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4275\n",
            "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4275/config.json\n",
            "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4275/pytorch_model.bin\n",
            "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4275/tokenizer_config.json\n",
            "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4275/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from bert-finetuned-sem_eval-english/checkpoint-3420 (score: 0.6775818639798489).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4275, training_loss=0.2923255920410156, metrics={'train_runtime': 682.126, 'train_samples_per_second': 50.123, 'train_steps_per_second': 6.267, 'total_flos': 2249123476753920.0, 'train_loss': 0.2923255920410156, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "pustzXiH63Q2"
      },
      "outputs": [],
      "source": [
        "text = \"I'm happy I can finally train a model for multi-label classification\"\n",
        "\n",
        "encoding = tokenizer(text, return_tensors=\"pt\")\n",
        "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "outputs = trainer.model(**encoding)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = outputs.logits\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOBEhZwOLsil",
        "outputId": "89abdd98-bdab-444b-b5c2-338dbcaa2be0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply sigmoid + threshold\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "probs = sigmoid(logits.squeeze().cpu())\n",
        "predictions = np.zeros(probs.shape)\n",
        "predictions[np.where(probs >= 0.5)] = 1\n",
        "# turn predicted id's into actual label names\n",
        "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNtCSeyqLt19",
        "outputId": "f9680dac-878d-47d1-b3eb-9d0e552d4050"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['joy', 'optimism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lj7Q2MUELu-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fine-tuning BERT (and friends) for multi-label text classification_English.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}